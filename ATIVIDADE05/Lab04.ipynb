{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8528 lines\n",
      "Sample line: (98521, u\"Maid in Manhattan might not look so appealing on third or fourth viewing down the road ... But as a high concept vehicle for two bright stars of the moment who can rise to fans ' lofty expectations , the movie passes inspection .\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parseRDD(point):\n",
    "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
    "        a sentence (third field).\n",
    "    Args:\n",
    "        point (str): input data point\n",
    "    Returns:\n",
    "        str: a string\n",
    "    \"\"\"    \n",
    "    data = point.split('\\t')\n",
    "    return (int(data[0]),data[2])\n",
    "\n",
    "def notempty(point):\n",
    "    \"\"\" Returns whether the point string is not empty\n",
    "    Args:\n",
    "        point (str): input string\n",
    "    Returns:\n",
    "        bool: True if it is not empty\n",
    "    \"\"\"   \n",
    "    return len(point[1])>0\n",
    "\n",
    "filename = os.path.join(\"/home/rafael/\",\"MovieReviews2.tsv\")\n",
    "rawRDD = sc.textFile(filename,100)\n",
    "header = rawRDD.take(1)[0]\n",
    "\n",
    "dataRDD = (rawRDD\n",
    "           #.sample(False, 0.1, seed=42)\n",
    "           .filter(lambda x: x!=header)\n",
    "           .map(parseRDD)\n",
    "           .filter(notempty)\n",
    "           #.sample( False, 0.1, 42 )\n",
    "           )\n",
    "\n",
    "print ('Read {} lines'.format(dataRDD.count()))\n",
    "print ('Sample line: {}'.format(dataRDD.takeSample(False, 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "stopfile = os.path.join(\"/home/rafael/\",\"stopwords.txt\")\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    words = re.split(string=string, pattern=split_regex)\n",
    "    return [w.lower() for w in words if w.lower() not in stopwords and len(w) > 0]\n",
    "\n",
    "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
    "\n",
    "print (wordsRDD.take(1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert wordsRDD.take(1)[0]==[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking'], 'lista incorreta!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.013683137483894825,0.003714576829224825,-0.1357858031988144,0.04758540168404579,0.04148530960083008]\n",
      "[(u'god', 0.9973623156547546), (u'shows', 0.9930634498596191)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "model = Word2Vec().setVectorSize(5).setSeed(42).fit(wordsRDD)\n",
    "\n",
    "print (model.transform(u'entertaining'))\n",
    "print (list(model.findSynonyms(u'entertaining', 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.abs(model.transform(u'entertaining')-np.array([0.0136831374839,0.00371457682922,-0.135785803199,0.047585401684,0.0414853096008])).mean()\n",
    "assert dist<1e-6, 'valores incorretos'\n",
    "assert list(model.findSynonyms(u'entertaining', 1))[0][0] == 'god', 'valores incorretos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3388 tokens únicos\n",
      "Vetor entertaining: [0.013683137483894825,0.003714576829224825,-0.1357858031988144,0.04758540168404579,0.04148530960083008]\n",
      "((5, 5), (10, 5))\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = (wordsRDD\n",
    "               .flatMap(lambda l:[(x,1) for x in l])\n",
    "               .reduceByKey(lambda x,y:x+y)\n",
    "               .filter(lambda x:x[1] >= 5)\n",
    "               .map(lambda x:x[0])\n",
    "               .collect()\n",
    "               )\n",
    "\n",
    "print ('{} tokens únicos'.format(len(uniqueWords)))\n",
    "\n",
    "w2v = {}\n",
    "for w in uniqueWords:\n",
    "    w2v[w] = model.transform(w)\n",
    "w2vb = sc.broadcast(w2v)  # acesse como w2vb.value[w]     \n",
    "print ('Vetor entertaining: {}'.format( w2v[u'entertaining']))\n",
    "\n",
    "vectorsRDD = (wordsRDD\n",
    "              .map(lambda xs:np.matrix([w2vb.value[x] for x in xs if x in uniqueWords]))\n",
    "             )\n",
    "recs = vectorsRDD.take(2)\n",
    "firstRec, secondRec = recs[0], recs[1]\n",
    "print (firstRec.shape, secondRec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(uniqueWords) == 3388,  'valor incorreto'\n",
    "assert np.mean(np.abs(w2v[u'entertaining']-[0.0136831374839,0.00371457682922,-0.135785803199,0.047585401684,0.0414853096008]))<1e-6,'valor incorreto'\n",
    "assert secondRec.shape == (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample vector: [array([ 0.01593647,  0.05436355, -0.0969881 ,  0.00835642,  0.06551303])]\n",
      "10 first clusters allocation: [38, 81, 7, 149, 87, 43, 57, 17, 153, 85]\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.mllib.clustering import KMeans\n",
    "\n",
    "vectors2RDD = sc.parallelize(np.array(list(w2v.values())),1)\n",
    "print ('Sample vector: {}'.format(vectors2RDD.take(1)))\n",
    "\n",
    "modelK = KMeans.train(seed=42, k=200, rdd=vectors2RDD)\n",
    "\n",
    "clustersRDD = vectors2RDD.map(lambda x:modelK.predict(x))\n",
    "print ('10 first clusters allocation: {}'.format(clustersRDD.take(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert clustersRDD.take(10)==[142, 83, 42, 0, 87, 52, 190, 17, 56, 0], 'valor incorreto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(64, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0.]))]\n"
     ]
    }
   ],
   "source": [
    "def quantizador(point, model, k, w2v):\n",
    "    key = point[0]\n",
    "    words = tokenize(point[1])\n",
    "    matrix = np.array( [w2v[x] for x in words if x in w2v] )\n",
    "    features = np.zeros(k)\n",
    "    for v in matrix:\n",
    "        c = model.predict(v)\n",
    "        features[c] += 1\n",
    "    return (key, features)\n",
    "    \n",
    "quantRDD = dataRDD.map(lambda x: quantizador(x, modelK, 500, w2v))\n",
    "\n",
    "print quantRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert quantRDD.take(1)[0][1].sum() == 5, 'valores incorretos'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
