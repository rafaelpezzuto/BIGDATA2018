{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.2 ms\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def parseRDD(point):\n",
    "    \"\"\" Converte um ponto de metricas da base de dados para uma tupla (id, vetor de floats).\n",
    "        Recebe um ponto onde o primeiro campo eh o id do vertice os 13 seguintes sao os valores para cada metrica\n",
    "        e retorna uma tupla composta pelo id e pelas 13 metricas (vetor de floats).\n",
    "    Args:\n",
    "        point (str): uma string onde os termos estao separados por ',', sendo o primeiro campo o id do vertice e \n",
    "        os 13 seguintes os valores para cada metrica\n",
    "    Returns:\n",
    "        (id, []): uma tupla composta pelo id do vertice e uma lista dos valores de 13 metricas\n",
    "    \"\"\"\n",
    "    data = point.split(';')\n",
    "    floatMetrics = [float(i) for i in data[1:]]\n",
    "    return (data[0], floatMetrics)\n",
    "\n",
    "def isNotZero(parsedPoint):\n",
    "    \"\"\" Retorna true se o ponto contem alguma metrica diferente de 0.\n",
    "    Args:\n",
    "        parsedPoint (str, []): uma tupla composta pelo id do vertice e lista de metricas\n",
    "    Returns:\n",
    "        bool: True se a lista contém pelo menos um valor nao nulo ou False se a somatoria da lista eh nula\n",
    "    \"\"\"\n",
    "    return sum(parsedPoint[1]) > 0\n",
    "\n",
    "def normalize(parsedPoint, means, standardDeviations, maxs, mins, normalizationType):\n",
    "    \"\"\" Normaliza um ponto. Recebe um ponto cujas valores maximo e minimo para as metricas \n",
    "        podem ser muito amplos e retorna um ponto cujas metricas estao entre 0 e 1.0.\n",
    "    Args:\n",
    "        parsedPoint (str, []): uma tupla composta pelo id do vertice e lista de metricas\n",
    "        means ([]): lista de medias das metricas\n",
    "        standardDeviations (list): lista de desvios-padrao das metricas\n",
    "        maxs ([]): lista de maximos das metricas\n",
    "        mins ([]): lista de minimos das metricas\n",
    "        normalizationType (str): tipo de normalizacao ('reescaling' | 'standard_score')\n",
    "    Returns:\n",
    "        (str, []): uma tupla de id (str) e metricas normalizadas ([])\n",
    "    \"\"\"\n",
    "    nodeId = parsedPoint[0]\n",
    "    metrics = parsedPoint[1]\n",
    "    numberOfMetrics = len(means) # numero de metricas == numero de medias == numero de desvios-padrao\n",
    "    if normalizationType == 'standard_score':\n",
    "        normalizedMetrics = [(metrics[i] - means[i])/standardDeviations[i] for i in range(numberOfMetrics)]\n",
    "    elif normalizationType == 'reescaling':\n",
    "         normalizedMetrics = [(metrics[i] - mins[i])/(maxs[i]-mins[i]) for i in range(numberOfMetrics)]\n",
    "    return (nodeId, normalizedMetrics)\n",
    "\n",
    "def euclidianDistance(pointA, pointB):\n",
    "    \"\"\" Calcula a distancia euclidiana entre dois pontos. Recebe dois pontos e retorna a distancia\n",
    "        euclidiana entre eles.\n",
    "    Args:\n",
    "        pointA ([]): lista de floats\n",
    "        pointB ([]): lista de floats\n",
    "    Returns:\n",
    "        float: a distancia entre dois pontos\n",
    "    \"\"\"\n",
    "    numberOfMetrics = len(pointA) # numero de metricas de A e B eh igual a 13\n",
    "    squaredDifferenceBetweenMetrics = [math.pow(pointA[i] - pointB[i], 2) for i in range(numberOfMetrics)]\n",
    "    return math.sqrt(sum(squaredDifferenceBetweenMetrics))\n",
    "\n",
    "def chooseRandomPoints(points, numberOfPoints, seed):\n",
    "    \"\"\" Escolhe um ponto aleatorio dentre uma lista de pontos. \n",
    "        Recebe uma lista de pontos e retorna um ponto dessa lista.\n",
    "    Args:\n",
    "        points (RDD): RDD de pontos\n",
    "        numberOfPoints (int): numero de pontos a serem escolhidos\n",
    "        seed (int): semente para permitir reobter mesmos dados aleatorios\n",
    "    Returns:\n",
    "        points ([[]]): uma lista de lista de metricas representando n pontos\n",
    "    \"\"\"\n",
    "    return points.takeSample(False, numberOfPoints, seed=seed)\n",
    "    \n",
    "def areCentroidsDifferent(pointA, pointB):\n",
    "    \"\"\" Verifica se dois pontos sao diferentes, com base nas listas de metricas.\n",
    "        Recebe dois pontos e returna True ou False.\n",
    "    Args:\n",
    "        pointA: um ponto do tipo [], em que contem metricas do tipo float\n",
    "        ponttB: um ponto do tipo [], em que contem metricas do tipo float\n",
    "    Returns:\n",
    "        bool: True se pointA[i] != pointB[i] para todo i ou False caso contrario\n",
    "    \"\"\"\n",
    "    for i in range(len(pointA)):\n",
    "        if pointA[i] != pointB[i]:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def kmeans(data, k, iteractions):\n",
    "    \"\"\" Base do bisect-kmeans. Recebe uma RDD de dados, um k=2 e o numero de iteracoes maximo e retorna dois clusters.\n",
    "    Args:\n",
    "        data (RDD): RDD de pontos do tipo (str, [])\n",
    "        k (int): numero de clusters a serem gerados. Padrao para bisect é 2 por vez\n",
    "        iteractions (int): numero de interacoes maximo\n",
    "    Returns:\n",
    "        clusters (RDD): um RDD contendo dois clusters. \n",
    "                        Cada ponto eh mapeado para (idClusters, idPonto, metricas)\n",
    "                        idClusters eh ou 0 ou 1\n",
    "                        metricas eh uma lista de floats\n",
    "    \"\"\"\n",
    "\n",
    "    # seleciona k pontos aleatorios e atribui a lista inicial de centroides\n",
    "    centroids = [p[1] for p in chooseRandomPoints(data, k, 42)] \n",
    "    \n",
    "    # roda kmeans ateh o limite de iteractions\n",
    "    for i in range(iteractions):\n",
    "        print('\\tkmeans interaction: %s' %(i + 1))\n",
    "        # calcula distancia entre cada ponto (segundo elemento da tupla) e os centroides, e atribui a cada ponto o id do centroide cuja distanca eh menor\n",
    "        clustersRDD = data.map(lambda x:(np.argmin([euclidianDistance(x[1], c) for c in centroids]), x[0], x[1]))\n",
    "        \n",
    "        # refaz lista de centroides, calculando o ponto medio dos pontos de cada cluster\n",
    "        newCentroids = (clustersRDD\n",
    "                        .map(lambda x:(x[0], [x[2],1])) # mapeia cada ponto para (indice do centroide, [vetor de metricas, 1])\n",
    "                        .reduceByKey(lambda x,y:([(np.array(x[0]) + np.array(y[0])),(x[1] + y[1])])) # reduz conjunto de pontos com mesmo indice do centroide para a somatoria dos vetores de metricas\n",
    "                        .map(lambda x:list(np.array(x[1][0])/(x[1][1]))) # divide a somatoria dos vetores pelo numero de pontos\n",
    "                       ).collect()\n",
    "        \n",
    "        # caso numero de centroides mude, seleciona centroides faltantes de forma aleatoria\n",
    "        # isso pode ocorrer se o reduceByKey da linha 108 nao tiver a chave de um dos centroids\n",
    "        if len(newCentroids) != len(centroids):\n",
    "            print('\\t\\tnumber of centroids has changed (fixing it now)')\n",
    "            diff = len(centroids) - len(newCentroids)\n",
    "            newCentroids.extend = [p[1] for p in chooseRandomPoints(data, diff, 84)]\n",
    "                \n",
    "        # verifica se centroides mudaram\n",
    "        centroidsHaveChanged = [areCentroidsDifferent(centroids[i], newCentroids[i]) for i in range(len(centroids))]\n",
    "        \n",
    "        if True not in centroidsHaveChanged:\n",
    "            print('\\t\\tcentroids have not changed')\n",
    "            return clustersRDD\n",
    "        else:\n",
    "            centroids = newCentroids\n",
    "    return clustersRDD\n",
    "\n",
    "def getClustersOrderedBySize(clusters):\n",
    "    \"\"\" Verifica qual dos clusters possui mais elementos e os retorna ordenado pela quantidade crescente de elementos.\n",
    "    Args:\n",
    "        clusters (RDD): RDD contendo pontos cujas chaves ou sao 0 ou sao 1\n",
    "    Returns:\n",
    "        clusterMenor (RDD), clusterMaior (RDD): Dois RDDs de pontos em que o primeiro eh o menor e o segundo o maior\n",
    "    \"\"\"\n",
    "    cluster1 = clusters.filter(lambda x:x[0] == 0)\n",
    "    cluster2 = clusters.filter(lambda x:x[0] == 1)\n",
    "    if cluster1.count() > cluster2.count():\n",
    "        return cluster2, cluster1\n",
    "    else:\n",
    "        return cluster1, cluster2\n",
    "    \n",
    "def bisectKmeans(data, k, iteractions, forceAllClustersWithPoints):\n",
    "    \"\"\" Algoritmo principal do bisect-kmeans. Recebe uma RDD de dados, o numero de clusters a serem gerados,\n",
    "        o numero de iteracoes maximo do subalgoritmo kmeans e um boleano indicando se pode haver clusters vazios\n",
    "    Args:\n",
    "        data (RDD): RDD de pontos do tipo (idPonto, metricas), onde idPonto eh um str e metricas eh uma lista de floats\n",
    "        k (int): numero de clusters\n",
    "        iteractions (int): numero de iteracoes do subalgoritmo k-means\n",
    "        forceAllClustersWithPoints (bool): um boleano que indica que nao sera permitido clusters vazios\n",
    "    Returns:\n",
    "        finalClusters ([]): lista de pontos do tipo (idPonto, metricas), \n",
    "                            len(finalClusters) eh garantido ser k, se forceAllClusterWithPoints for True\n",
    "                            cada cluster contem x pontos\n",
    "    \"\"\"\n",
    "    # inicializa lista final de clusters\n",
    "    finalClusters = []\n",
    "    \n",
    "    # todos os pontos a serem divididos\n",
    "    clusterToSplit = data\n",
    "    i = 0\n",
    "    \n",
    "    while(i != k - 1):\n",
    "        print('bisect interaction: %s - clusters: %s' %(i + 1, len(finalClusters)))\n",
    "        # roda kmeans para k = 2\n",
    "        dualClustersRDD = kmeans(data=clusterToSplit, k=2, iteractions=iteractions)\n",
    "\n",
    "        # melhor cluster eh o menor e o pior cluster eh o maior\n",
    "        bestClusterRDD, worstClusterRDD = getClustersOrderedBySize(dualClustersRDD)\n",
    "        \n",
    "        print('size(best cluster) %s' %bestClusterRDD.count())\n",
    "        print('size(worst cluster) %s' %worstClusterRDD.count())\n",
    "        \n",
    "        if forceAllClustersWithPoints and bestClusterRDD.count() == 0:\n",
    "            print('found empty cluster - repeating proccess')\n",
    "        else:\n",
    "            # insere melhor cluster na lista final de clusters\n",
    "            finalClusters.append(bestClusterRDD)\n",
    "\n",
    "            # incrementa numero de clusters\n",
    "            i += 1\n",
    "\n",
    "            # insere o pior cluster para rodar novamente no kmeans\n",
    "            clusterToSplit = worstClusterRDD.map(lambda x:(x[1],x[2]))\n",
    "        \n",
    "    finalClusters.append(worstClusterRDD)        \n",
    "    return finalClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "fileName = ('metricas.csv')\n",
    "rawRDD = sc.textFile(fileName)\n",
    "metricsHeader = rawRDD.take(1)[0]\n",
    "metricsRDD = (rawRDD\n",
    "              .filter(lambda x: x != metricsHeader)\n",
    "              .map(lambda x:parseRDD(x))\n",
    "              .filter(lambda x: isNotZero(x))\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "maxs = []\n",
    "mins = []\n",
    "stdevs = []\n",
    "for i in range(13):\n",
    "    metricI = metricsRDD.map(lambda x: x[1][i])\n",
    "    maxs.append(metricI.max())\n",
    "    mins.append(metricI.min())\n",
    "    means.append(metricI.mean())\n",
    "    stdevs.append(metricI.stdev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.95 ms\n"
     ]
    }
   ],
   "source": [
    "normalizedMetricsRDD = metricsRDD.map(lambda x:normalize(x, means, stdevs, maxs, mins, 'standard_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bisect interaction: 1 - clusters: 0\n",
      "\tkmeans interaction: 1\n",
      "\tkmeans interaction: 2\n",
      "\tkmeans interaction: 3\n",
      "\tkmeans interaction: 4\n",
      "\tkmeans interaction: 5\n",
      "\tkmeans interaction: 6\n",
      "\tkmeans interaction: 7\n",
      "\tkmeans interaction: 8\n",
      "\tkmeans interaction: 9\n",
      "\tkmeans interaction: 10\n",
      "\tkmeans interaction: 11\n",
      "\tkmeans interaction: 12\n",
      "\tkmeans interaction: 13\n",
      "\tkmeans interaction: 14\n",
      "\tkmeans interaction: 15\n",
      "\tkmeans interaction: 16\n",
      "\tkmeans interaction: 17\n",
      "\tkmeans interaction: 18\n",
      "\tkmeans interaction: 19\n",
      "\tkmeans interaction: 20\n",
      "size(best cluster) 505293\n",
      "size(worst cluster) 600312\n",
      "bisect interaction: 2 - clusters: 1\n",
      "\tkmeans interaction: 1\n",
      "\tkmeans interaction: 2\n",
      "\tkmeans interaction: 3\n",
      "\tkmeans interaction: 4\n",
      "\tkmeans interaction: 5\n",
      "\tkmeans interaction: 6\n",
      "\tkmeans interaction: 7\n",
      "\tkmeans interaction: 8\n",
      "\tkmeans interaction: 9\n",
      "\tkmeans interaction: 10\n",
      "\tkmeans interaction: 11\n",
      "\tkmeans interaction: 12\n",
      "\tkmeans interaction: 13\n",
      "\tkmeans interaction: 14\n",
      "\tkmeans interaction: 15\n",
      "\tkmeans interaction: 16\n",
      "\tkmeans interaction: 17\n",
      "\tkmeans interaction: 18\n",
      "\tkmeans interaction: 19\n",
      "\tkmeans interaction: 20\n",
      "size(best cluster) 147116\n",
      "size(worst cluster) 453196\n",
      "bisect interaction: 3 - clusters: 2\n",
      "\tkmeans interaction: 1\n",
      "\tkmeans interaction: 2\n",
      "\tkmeans interaction: 3\n",
      "\tkmeans interaction: 4\n",
      "\tkmeans interaction: 5\n",
      "\tkmeans interaction: 6\n",
      "\tkmeans interaction: 7\n",
      "\tkmeans interaction: 8\n",
      "\tkmeans interaction: 9\n",
      "\tkmeans interaction: 10\n",
      "\tkmeans interaction: 11\n",
      "\tkmeans interaction: 12\n",
      "\tkmeans interaction: 13\n",
      "\tkmeans interaction: 14\n",
      "\tkmeans interaction: 15\n",
      "\tkmeans interaction: 16\n",
      "\tkmeans interaction: 17\n",
      "\t\tcentroids have not changed\n",
      "size(best cluster) 142446\n",
      "size(worst cluster) 310750\n",
      "bisect interaction: 4 - clusters: 3\n",
      "\tkmeans interaction: 1\n",
      "\tkmeans interaction: 2\n",
      "\tkmeans interaction: 3\n",
      "\tkmeans interaction: 4\n",
      "\tkmeans interaction: 5\n",
      "\tkmeans interaction: 6\n",
      "\tkmeans interaction: 7\n",
      "\tkmeans interaction: 8\n",
      "\tkmeans interaction: 9\n",
      "\tkmeans interaction: 10\n",
      "\tkmeans interaction: 11\n",
      "\tkmeans interaction: 12\n",
      "\tkmeans interaction: 13\n",
      "\tkmeans interaction: 14\n",
      "\tkmeans interaction: 15\n",
      "\tkmeans interaction: 16\n",
      "\tkmeans interaction: 17\n",
      "\tkmeans interaction: 18\n",
      "\tkmeans interaction: 19\n",
      "\tkmeans interaction: 20\n",
      "size(best cluster) 54910\n",
      "size(worst cluster) 255840\n",
      "time: 56min 30s\n"
     ]
    }
   ],
   "source": [
    "clusters = bisectKmeans(data=normalizedMetricsRDD, iteractions=20, k=5, forceAllClustersWithPoints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 1 possui 505293 pontos\n",
      "cluster 2 possui 147116 pontos\n",
      "cluster 3 possui 142446 pontos\n",
      "cluster 4 possui 54910 pontos\n",
      "cluster 5 possui 255840 pontos\n",
      "------------------------\n",
      "total possui 1105605 pontos\n",
      "time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "tamanhos = []\n",
    "for index, c in enumerate(clusters):\n",
    "    tamanhos.append(c.count())\n",
    "    print('cluster %s possui %s pontos' %(index + 1, tamanhos[index]))\n",
    "    c.saveAsTextFile('cluster_t12_k5/%s' %index)\n",
    "    \n",
    "print('------------------------\\ntotal possui %s pontos' %sum(tamanhos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
